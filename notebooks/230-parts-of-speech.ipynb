{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts-Of-Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will define a file for analysis, and the script will output interesting features & characteristics of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "CARREL   = 'homer'\n",
    "KEYWORD  = 'love'\n",
    "KEYWORDS = [ 'love', 'war', 'man' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require\n",
    "from nltk import *\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "library = Path( LIBRARY )\n",
    "text = library/CARREL/ETC/TEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the given file, and do a bit of normalization against it\n",
    "with open ( text ) as handle : text = handle.read()\n",
    "text = text.replace( '\\t', '').replace( '\\n', '' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all sentences\n",
    "sentences = sent_tokenize( text )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denote a sentence and output it\n",
    "S = 4\n",
    "sentences[ 4 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a new list of sentences containing the given keyword\n",
    "sentences = [ sentence for sentence in sentences if KEYWORD in sentence ] \n",
    "for sentence in sentences :\n",
    "    print( sentence )\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify a sentence of interest, tokenize it, and extract parts-of-speech\n",
    "S    = 5\n",
    "tags = pos_tag( word_tokenize( sentences[ S ] ) )\n",
    "\n",
    "# output\n",
    "tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple grammar, initialize a parser, parse the tags, and output\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser  = RegexpParser( grammar )\n",
    "parse   = parser.parse( tags )\n",
    "print( parse )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count & tabulate the parts-of-speech; begin by initializing\n",
    "pos = {}\n",
    "\n",
    "# process each tag\n",
    "for tag in tags :\n",
    "    \n",
    "    # parse\n",
    "    tag = tag[ 1 ]\n",
    "    \n",
    "    # update the list of pos tags\n",
    "    if tag in pos : pos[ tag ] += 1\n",
    "    else : pos[ tag ] = 1\n",
    "\n",
    "# sort the list; very Pythonic\n",
    "pos = { key:value for key, value in sorted( pos.items(), key=lambda item:item[ 1 ], reverse=True ) }\n",
    "\n",
    "# output\n",
    "print( \"\\t\".join( ( 'pos', 'count') ) )\n",
    "for tag in pos :\n",
    "    count = str( pos[ tag ] )\n",
    "    print( \"\\t\".join( ( tag, count ) ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denote a part-of-speech tag, and output all words with that tag\n",
    "P = 'NN'\n",
    "for tag in tags :\n",
    "    if ( tag[ 1 ] == P ) : print ( tag[ 0 ] )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count & tabulate all parts-of-speech from the given file (data)\n",
    "tags = pos_tag( word_tokenize( text ) )\n",
    "\n",
    "# process each tag\n",
    "for tag in tags :\n",
    "    \n",
    "    # parse\n",
    "    tag = tag[ 1 ]\n",
    "    \n",
    "    # update the list of pos tags\n",
    "    if tag in pos : pos[ tag ] += 1\n",
    "    else          : pos[ tag ] =  1\n",
    "\n",
    "# sort the list; very Pythonic\n",
    "pos = { key:value for key, value in sorted( pos.items(), key=lambda item:item[ 1 ], reverse=True ) }\n",
    "\n",
    "# output\n",
    "print( \"\\t\".join( ( 'pos', 'count') ) )\n",
    "for tag in pos :\n",
    "    count = str( pos[ tag ] )\n",
    "    print( \"\\t\".join( ( tag, count ) ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denote a part-of-speech tag, and output all words with that tag; initialize\n",
    "P      = 'VB'\n",
    "tokens = {}\n",
    "\n",
    "# process each tag\n",
    "for tag in tags :\n",
    "    \n",
    "    # check for given part of speech\n",
    "    if ( tag[ 1 ] == P ) :\n",
    "    \n",
    "        # update the list of pos tags\n",
    "        if tag[ 0 ] in tokens : tokens[ tag[ 0 ] ] += 1\n",
    "        else : tokens[ tag[ 0 ] ] = 1\n",
    "\n",
    "# sort the list; very Pythonic\n",
    "tokens = { key:value for key, value in sorted( tokens.items(), key=lambda item:item[ 1 ], reverse=True ) }\n",
    "\n",
    "# output\n",
    "print( \"\\t\".join( ( 'token', 'count' ) ) )\n",
    "for token in tokens :\n",
    "    \n",
    "    # parse and output\n",
    "    count = str( tokens[ token ] )\n",
    "    print( \"\\t\".join( ( token, count ) ) )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a list of frequencies\n",
    "frequencies = []\n",
    "for token in tokens : frequencies.append( ( tokens[ token ] ) )\n",
    "\n",
    "# plot the result\n",
    "plt.hist( frequencies, bins=(max(frequencies)-min(frequencies)) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean (average), variance, and standard deviation\n",
    "mean        = sum( frequencies ) / len( frequencies )\n",
    "variance    = sum( ( frequency-mean )**2 for frequency in frequencies ) / len(frequencies)\n",
    "deviation   = variance**0.5\n",
    "print( \"mean: %f; variance: %f; deviation: %f\" % ( mean, variance, deviation ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a list of second teir \"interesting\" words\n",
    "\n",
    "# initialize\n",
    "n = round( mean + deviation )\n",
    "frequencies = {}\n",
    "for token in tokens :\n",
    "\n",
    "        # re-initialize\n",
    "        count = tokens[ token ]\n",
    "        \n",
    "        # optionally update\n",
    "        if ( mean <= count <= n ) : frequencies[ token ] = count\n",
    "\n",
    "# output\n",
    "print( frequencies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialilze a word cloud,...\n",
    "wordcloud = WordCloud( width=WIDTH, height=HEIGHT, background_color=COLOR )\n",
    "\n",
    "# ...render it, and display it\n",
    "plt.imshow( wordcloud.generate_from_frequencies( frequencies ) ) \n",
    "plt.axis( \"off\" ) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
