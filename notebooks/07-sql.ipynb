{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study carrels, and SQL, and Python (\"Oh my!\")\n",
    "\n",
    "The result of the Distant Reader process is the creation of a \"study carrel\" -- a structured data set with many components. One of those components is an SQLite database file. This notebook outlines many ways to extract information from the database and output the result in a number of different ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize\n",
    "\n",
    "The first steps are to: 1) configure what database to read, import SQLite functionality into the script, and to open (\"initialize\") a connection to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-configure\n",
    "LIBRARY = '/Users/eric/Documents/reader-library'\n",
    "CARREL  = 'homer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "ETC      = 'etc'\n",
    "DATABASE = 'reader.db'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require\n",
    "import sqlite3\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "library                = Path( LIBRARY )\n",
    "database               = library/CARREL/ETC/DATABASE\n",
    "connection             = sqlite3.connect( database )\n",
    "connection.row_factory = sqlite3.Row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bibliographics\n",
    "\n",
    "The database includes a table called \"bib\" for \"bibliographics\". This is the study carrel's central table, and it includes fields for things like identifier, author, title, date, and summary (a computed \"abstract\"). It also includes fields akin to values for extent, such as: number of words, number of sentences, and readability score. It also include fields denoting the location of the cached original documents as well as their plain text transformations.\n",
    "\n",
    "The following cells outline how to query the bib (\"bibliographics\") table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many items are in the database; initialize a query\n",
    "sql = \"SELECT COUNT( id ) FROM bib\"\n",
    "\n",
    "# search; there is only one result, so only get a single item\n",
    "result = connection.execute( sql ).fetchone()\n",
    "\n",
    "# parse the result\n",
    "count = result[ 0 ]\n",
    "\n",
    "# output a formatted message\n",
    "print( \"There are %d documents in the database.\" % count )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what is the average readability score of all documents; initialize and search\n",
    "sql    = \"SELECT CAST( AVG( flesch ) AS INTEGER ) FROM bib\"\n",
    "result = connection.execute( sql ).fetchone()\n",
    "\n",
    "# parse the result\n",
    "score = result[ 0 ]\n",
    "\n",
    "# output a formatted message\n",
    "print( \"The average readability score is %d.\" % score )\n",
    "print( \"Scores closer to 100 are easier to read. Scores closer to zero are more difficult.\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a rudimentary bibliography; initialize\n",
    "header = [ 'id', 'author', 'title', 'date' ]\n",
    "sql    = \"SELECT id, author, title, date FROM bib ORDER BY author\"\n",
    "\n",
    "# search; find all rows\n",
    "rows = connection.execute( sql )\n",
    "\n",
    "# output the header\n",
    "print( \"\\t\".join( header ) )\n",
    "\n",
    "# process each row; output a tab-delimited list\n",
    "for row in rows :\n",
    "    \n",
    "    id     = str( row[ 'id' ] )\n",
    "    author = str( row[ 'author' ] )\n",
    "    title  = str( row[ 'title' ] )\n",
    "    date   = str( row[ 'date' ] )\n",
    "    \n",
    "    print( '\\t'.join( [ id, author, title, date ]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a rudimentary bibliography with keywords and summary; configure and find all\n",
    "sql  = '''SELECT\n",
    "            b.id,\n",
    "            b.author,\n",
    "            b.title,\n",
    "            b.date,\n",
    "            GROUP_CONCAT( w.keyword, '; ' ) AS keywords,\n",
    "            b.summary\n",
    "          FROM\n",
    "            bib AS b,\n",
    "            wrd AS w\n",
    "          WHERE\n",
    "            b.id = w.id\n",
    "          GROUP BY\n",
    "            b.id\n",
    "          ORDER BY\n",
    "            b.author'''\n",
    "rows = connection.execute( sql )\n",
    "\n",
    "# process each row\n",
    "for row in rows : \n",
    "    \n",
    "    # parse\n",
    "    id, author, title, date, keywords, summary = row\n",
    "    \n",
    "    # output\n",
    "    print( \"          id: %s\" % id )\n",
    "    print( \"      author: %s\" % author )\n",
    "    print( \"       title: %s\" % title )\n",
    "    print( \"        date: %s\" % date )\n",
    "    print( \"  keyword(s): %s\" % keywords )\n",
    "    print( \"     summary: %s\" % summary )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-of-speech, tokens, and words\n",
    "\n",
    "The largest table in a study carrel -- by far -- is the pos (\"parts-of-speech\") table. This table contains each & every word from each & every document in a study carrel.\n",
    "\n",
    "Each row in the pos table describes a \"token\", and a token may be a word, a number, a puncutation mark, or a combination of any of those things. Each token is assoicated with a bib (\"document\") id, a sentence id, a token id, the token, the token's lemma, and the token's part-of-speech label (\"NN\" for noun, \"VRB\" for verb, \"JJ\" for adjective, etc.)\n",
    "\n",
    "Given this data structure, it is possible to count & tabulate the frequency of words, word stems, the lemmas of words, and parts-of-speech. Given a word, word stem, lemma, or part-of-speech value, it is also possible to extract and rebuild all the sentences containing these values. So, for example, the student, researcher, or scholar can output all the sentences containing \"ahad\" and/or \"whale\", and then they can do analysis against the result.\n",
    "\n",
    "It is possible to apply combinations of SQL and grammars to the pos table, but such is discouraged. Instead the student, researcher, or scholar is encouraged to use alternative pattern-matching and/or machine learning techniques. Such techniques are implemented in the veneragble the Natural Langauge Toolkit, spaCy, and Textacy Python libraries.\n",
    "\n",
    "The follow cells describe a number of different -- and hopefully, interesting -- techniques for exploiting the pos table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "Each document in the study carrel is associated with zero or more statistically computed keywords. These keywords are stored in a table called \"wrd\", and the table only has two fields: 1) id, and 2) keyword. The value of id is the value of a bib table id, and it is through this value that SQL joins can be established.\n",
    "\n",
    "The cells below outline ways the wrd (\"keywords\") table can be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many keywords are in this carrel; initialize and get the result\n",
    "sql     = \"select count( keyword ) from wrd\"\n",
    "results = connection.execute( sql ).fetchone()\n",
    "\n",
    "# parse\n",
    "count = results[ 0 ]\n",
    "\n",
    "# output a formatted message\n",
    "print( \"There are %d keywords in this carrel.\" % count )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many distinct keywords exist in this carrel; initialize and search\n",
    "sql     = \"select count( distinct( lower( keyword ) ) ) from wrd\"\n",
    "results = connection.execute( sql ).fetchone()\n",
    "\n",
    "# parse\n",
    "count = results[ 0 ]\n",
    "\n",
    "# output a formatted message\n",
    "print( \"There are %d distinct (read \\\"unique\\\") keywords in this carrel.\" % count )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count and tabulate the keywords; so, what are the keywords and how often do they occur?\n",
    "\n",
    "# configure and search\n",
    "header = ( 'count', 'keyword' )\n",
    "sql    = '''select\n",
    "              lower(keyword),\n",
    "              count(lower(keyword)) as count\n",
    "            from\n",
    "              wrd\n",
    "            group by\n",
    "              lower(keyword)\n",
    "            order by\n",
    "               count desc'''\n",
    "rows   = connection.execute( sql )\n",
    "\n",
    "# output a header\n",
    "print( \"\\t\".join( header ) )\n",
    "\n",
    "# process each result\n",
    "for row in rows :\n",
    "    \n",
    "    # parse and output as a tab-delimited list\n",
    "    keyword, count = row\n",
    "    print( \"\\t\".join ( ( str( count ), keyword ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list items with a given keyword\n",
    "\n",
    "# initialize; denote a keyword from the output of the previous cell\n",
    "keyword = 'love'\n",
    "\n",
    "# build a query and execute it; sounds so brutal\n",
    "sql = ( '''select\n",
    "             b.title\n",
    "           from\n",
    "             bib as b,\n",
    "             wrd as w\n",
    "           where\n",
    "             lower(keyword) is '%s'\n",
    "             and\n",
    "             b.id = w.id\n",
    "           order by\n",
    "             title''' % keyword )\n",
    "rows = connection.execute( sql )\n",
    "\n",
    "# process each row; output a simple list\n",
    "for row in rows :\n",
    "    print( row[ 0 ] )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find documents with more than one given keyword; perform a Boolean intersection\n",
    "\n",
    "# configure with keywords from above, and remember, there may be zero documents in the result\n",
    "keyword01 = 'elizabeth'\n",
    "keyword02 = 'darcy'\n",
    "\n",
    "# initialize\n",
    "sql = ('''select \n",
    "           b.title,\n",
    "           group_concat(lower(w.keyword), '; ') as keywords\n",
    "         from\n",
    "           bib as b,\n",
    "           wrd as w,\n",
    "           wrd as w1,\n",
    "           wrd as w2\n",
    "         where\n",
    "           ( lower(w1.keyword) is '%s' and b.id is w1.id )\n",
    "           and\n",
    "           ( lower(w2.keyword) is '%s' and b.id is w2.id )\n",
    "           and b.id = w.id\n",
    "         group by\n",
    "           b.id\n",
    "         order by title''' % ( keyword01, keyword02 ) )\n",
    "\n",
    "# search\n",
    "rows = connection.execute( sql )\n",
    "\n",
    "# process each resulting row\n",
    "for row in rows :\n",
    "    \n",
    "    # parse\n",
    "    title    = row[ \"title\" ]\n",
    "    keywords = row[ \"keywords\" ] \n",
    "\n",
    "    # output\n",
    "    print( \"     title: %s\" % title )\n",
    "    print( \"  keywords: %s\" % keywords )\n",
    "    print()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs\n",
    "\n",
    "Many documents include URLs, and the Reader does its best to identify those URLs and store them in a table called \"urls\". The table includes three fields: 1) id, 2) url, and 3) domain. The value of id is a link back to the bib table. The value of url is the... URL. The domain value is the string after the initial \"//\" of a URL and before the first instance of \"/\". \n",
    "\n",
    "The cells below demonstrate some of the ways the url (\"URLs\") table can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many URLs are in this carrel; initialize and search\n",
    "sql = \"select count( url ) from url\"\n",
    "results = connection.execute( sql ).fetchone()\n",
    "\n",
    "count = results[ 0 ]\n",
    "\n",
    "print( \"There are %d URLs in this carrel.\" % count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count & tabulate the URLs; initialize and search\n",
    "header = ( 'count', 'url' )\n",
    "sql    = \"select url, count(url) as count from url group by url order by count desc\"\n",
    "rows   = connection.execute( sql )\n",
    "\n",
    "# output a header\n",
    "print( \"\\t\".join( header ) )\n",
    "\n",
    "# process each row\n",
    "for row in rows :\n",
    "    \n",
    "    # parse and output\n",
    "    url, count = row\n",
    "    print( \"\\t\".join( ( str( count), url ) ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique domains are represented by the URLs; do the work\n",
    "sql     = \"select count( distinct( lower( domain ) ) ) from url\"\n",
    "results = connection.execute( sql ).fetchone()\n",
    "print( \"There are %d unique domains represented by the URLs in this carrel.\" % results[ 0 ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count & tabulate the domains; what domains are oft-mentioned\n",
    "\n",
    "# configure and serach\n",
    "header = ( 'count', 'domain' )\n",
    "sql    = '''select\n",
    "              lower( domain ),\n",
    "              count( lower( domain ) ) as count\n",
    "            from\n",
    "              url\n",
    "            group by\n",
    "              lower( domain )\n",
    "            order by\n",
    "              count desc'''\n",
    "rows   = connection.execute( sql )\n",
    "\n",
    "# ouput a header, and process each row\n",
    "print( \"\\t\".join( header ) )\n",
    "for row in rows :\n",
    "    \n",
    "    # parse, and output some more\n",
    "    domain, count = row\n",
    "    print( \"\\t\".join( ( str( count), domain )))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "As a next step, return to the top of this notebook, change the value of \"DB\" to the name of a different database file found in this notebook's ./dbs directory. Examples include \"homer.db\", \"melville-moby-1851.db\" or \"shakespeare-sonnets.db\". Once you have changed the value, restart the notebook, and walk thorugh it again. By doing so the concepts outlined here will be re-enforced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
