{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gentle introduction to text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will do the simpliest of text mining against a file. The notebook employs only functionality which comes \"out of the box\" with Python; no additional libraries are imported. In the end, you will be able to do the most rudimentary comparisons between a number of different texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to denote a constant, the name of the file you want to \"read\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-configure\n",
    "LIBRARY = '/Users/eric/Documents/reader-library'\n",
    "CARREL  = 'homer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "ETC       = 'etc'\n",
    "TEXT      = 'reader.txt'\n",
    "STOPWORDS = 'stopwords.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to open the file, get all the data, and output the result. You can see that the file is a whole lot of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "library = Path( LIBRARY )\n",
    "text    = library/CARREL/ETC/TEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text\n",
    "with open( text ) as handle : text = handle.read()\n",
    "print( text )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to parse the data into tokens (\"words\"), and in this case, a token is defined as anything delimited by a space -- the default option of the \"split\" method. The result of this process will be the creation of a list (\"array\"), and the list is then output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()\n",
    "\n",
    "print()\n",
    "print( 'Your text consists of the following tokens (\"words\"):' )\n",
    "print()\n",
    "print( tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays have sizes -- the number of elements in the array. Here we calculate the size of the array and output it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len( tokens )\n",
    "\n",
    "print()\n",
    "print( 'The given file is %s tokens (\"words\") in size.' % size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to count & tabulate the items in the array. This will give us a set of name-value pairs (a type of Python list called a \"dictionary\") where the name of each pair is a token, and the value of each pair is the token's frequency. Once the name-values pairs have been created, the dictionary is sorted by value, and finally we output the N most frequently occuring tokens along with their counts.\n",
    "\n",
    "The sorting process is very \"Pythonic\", meaning, the process uses an idiom specific to the Python programming langauge. \n",
    "\n",
    "Increase or decrease the value of N to output a greater or lesser number of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = {}\n",
    "N           = 50\n",
    "\n",
    "for token in tokens :\n",
    "    \n",
    "    if token in frequencies : frequencies[ token ] += 1\n",
    "    else                    : frequencies[ token ] =  1\n",
    "\n",
    "frequencies = { key:value for key, value in sorted( frequencies.items(), key=lambda item:item[ 1 ], reverse=True ) }\n",
    "\n",
    "print()\n",
    "print( 'The %i most frequently occuring tokens and their counts are:' % N )\n",
    "print()\n",
    "\n",
    "for index, token in enumerate( frequencies ) :\n",
    "            \n",
    "    count = frequencies[ token ]\n",
    "    print( '  * %s (%i)' % ( token, count ) )\n",
    "    \n",
    "    if ( index > N ) : break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the size of N (above) is large enough, you will see tokens containing punctation marks, such as the tokens at the end of sentences. If the text was a more normal text (one that has not been preprocessed by the Distant Reader), then you would also see words in a variety of cases. Accurate text mining requires the tokens to be normalized (\"cleaned\"), and the following cell illustrates how to retain word-only tokens as well as lower-casing all tokens all in a single go. This process is also very Pythonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ token.lower() for token in tokens if token.isalpha() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost without a doubt, the list of most frequent words included words of little interest, such as \"the\", \"a\", or \"an\". These types of words are often called \"stop words\". The following cell initializes a list of stop words and then removes them from the list of tokens. Yet again, the process is Pythonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = library/CARREL/ETC/STOPWORDS\n",
    "with open( stopwords ) as handle : stopwords = handle.read().split( '\\n')\n",
    "tokens    = [ token for token in tokens if token not in stopwords ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of tokens has now been altered, and consequently you will want to re-create a list of frequencies, sort the list, and output the N most frequent tokens. The result ought to more meaningful than the previous list of frequencies.\n",
    "\n",
    "Increase or decrease the value of N to output a greater or lesser number of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frequencies = {}\n",
    "N           = 50\n",
    "\n",
    "for token in tokens :\n",
    "    \n",
    "    if token in frequencies : frequencies[ token ] += 1\n",
    "    else                    : frequencies[ token ] =  1\n",
    "\n",
    "frequencies = { key:value for key, value in sorted( frequencies.items(), key=lambda item:item[ 1 ], reverse=True ) }\n",
    "\n",
    "print()\n",
    "print( 'The %i most frequently occuring tokens and their counts are:' % N )\n",
    "print()\n",
    "\n",
    "for index, token in enumerate( frequencies ) :\n",
    "            \n",
    "    count = frequencies[ token ]\n",
    "    print( '  * %s (%i)' % (token, count ))\n",
    "    \n",
    "    if ( index > N ) : break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates the simplest of visualizations illustrating the frequency of the most frequent N tokens. Season the value of N to taste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N       = 50\n",
    "width   = 50\n",
    "maximum = round( frequencies[ list( frequencies.keys() )[ 0 ] ] * 1.1 )\n",
    "\n",
    "for index, token in enumerate( frequencies ) :\n",
    "    \n",
    "    count = frequencies[ token ]\n",
    "    size  = round ( ( count * width / maximum ) )\n",
    "    print( token, \"\\t\", '#' * size )\n",
    "    \n",
    "    if ( index > N ) : break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite probable you will want to know the significance of given idea in your text(s). For example, you might be interested in \"love\" or \"war\" or \"truth\" or \"beauty\". One way to determine significance is to compare the frequency of the given idea to the size of the entire text. The greater the resulting ratio is the more significant the given word.\n",
    "\n",
    "Change the value of idea to observe differences between words, but be careful. Your idea may not appear in the text, and if it doesn't then a \"KeyError\" will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idea         = 'love'\n",
    "count        = frequencies[ idea ]\n",
    "size         = len( tokens )\n",
    "significance = count / size\n",
    "\n",
    "print()\n",
    "print( 'The significance of the idea \"%s\" is %f.' % ( idea, significance ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Text mining is a lot about reading files, normalizing the text, counting & tabulating tokens (\"words\"), comparing those tokens and their characteristics to other tokens, and visualizing the results. This notebook -- a gentle introduction to text mining -- demonstrated some of these techniques sans any special funcationality provided by any Python library.\n",
    "\n",
    "But that is doing it the hard way. There exist a whole world of text mining & natural langauge processing toolkits for analyzing (\"reading\") texts. The balance of notebooks in this repository exploit those toolkits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Consider some next steps.\n",
    "\n",
    "This repository came with a number of different Distant Reader study carrels. By default, this notebook processed content from the carrel named \"homer\". Other carrels in this repository ought to include:\n",
    "\n",
    "   * augustine-confessions-397\n",
    "   * melville-moby-1851\n",
    "   * shakespeare-sonnets\n",
    " \n",
    "Replace \"homer\" in the value of FILE at the beginning of this notebook to the name of a different study carrel, and then run the whole notebook again. Once you do so, then ask yourself the following questions:\n",
    "\n",
    "   * Which study carrel is largest? Smallest? How do they all compare?\n",
    "   * Depending on the carrel, how might you alter the list of stop words?\n",
    "   * How does the significance of a given idea compare across carrels?\n",
    "\n",
    "Finally, you are not limited to files from Distant Reader study carrels. Change the value of FILE to point to any plain text (.txt) file on your computer, and do a bit of additional \"reading\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<p style='text-align: right'>Eric Lease Morgan &lt;emorgan@nd.edu&gt;<br />\n",
    "April 21, 2021</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
